{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font color=black>\n",
    "\n",
    "**What?** Variational autoencoder on CIFAR-10\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- Note on module installation (as per MacOS)\n",
    "    - `pip install pytorch-lightning`\n",
    "    - `pip install pytorch-lightning-bolts`\n",
    "    - `pip install wandb`\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from pl_bolts.models.autoencoders.components import (\n",
    "    resnet18_decoder,\n",
    "    resnet18_encoder,\n",
    ")\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from matplotlib.pyplot import imshow, figure\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = CIFAR10DataModule('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Loss\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- The VAE uses a loss function called ELBO.\n",
    "- **ELBO** stands for Evidence Lower Bound\n",
    "- This is made of two parts as in:\n",
    "\n",
    "$$\n",
    "ELBO = \\color{red}{KL} + \\color{blue}{reconstruction} \\\\ \n",
    "= \\color{red}{\\min \\mathbb{E}_{q}[ \\log q(z|x) - \\log p(z)]} - \\color{blue}{\\mathbb{E}_{q} \\log p(x|z)}\n",
    "$$\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO and Monte Carlo Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- Most tutorials assume that `p(z)`, the prior is gaussian and also assumes that `p(z|x)` is also gaussian. This come handy as the KL is then known by a closed-form solution. So, most tutorials end up with a KL divergence that looks like:\n",
    "\n",
    "$$\n",
    "-0.5 \\sum{1 \\log{(\\sigma)} - \\mu^2 - e^{\\log{(\\sigma)}}}\n",
    "$$\n",
    "\n",
    "\n",
    "- But if we want to retain the flexibility to modify distributions as needed, we then need another strategy. Since we don't know the KL between all possible pairs of distributions, we'll actually just use **Monte-Carlo sampling** for this.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- KL is used to measure how different two distribution are.\n",
    "- You can also see it in another way: KL Divergence helps us to measure just how much information we lose when we choose an approximation, thus we can even use it as our objective function to pick which approximation would work best for the problem at hand.\n",
    "- In this cell we'll create two normal distributions and we'll get the value of KL. Then we'll modify the second one by making it more similar to the first. Upon recomputing the KL you'll see how the KL value is lowered. This means the two distibutiona re becoming more similar.\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5350)\n",
      "log prob pz:  tensor(-7.1670) prob: tensor(0.0008)\n",
      "log prob qzx:  tensor(-2.3789) prob: tensor(0.0927)\n"
     ]
    }
   ],
   "source": [
    "# Let's create a normal distribution of p the prior \n",
    "p = torch.distributions.Normal(0, 1)\n",
    "# Let's create a normal distribution of q, the posterior\n",
    "q = torch.distributions.Normal(2, 4)\n",
    "\n",
    "# Let's draw a sample (z) from the q distribution\n",
    "z = q.rsample()\n",
    "print(z)\n",
    "\n",
    "# Log propability of z under p distribution\n",
    "log_pz = p.log_prob(z)\n",
    "# Log propability of z under q distribution\n",
    "log_qzx = q.log_prob(z)\n",
    "\n",
    "print('log prob pz: ', log_pz, 'prob:', torch.exp(log_pz))\n",
    "print('log prob qzx: ', log_qzx, 'prob:', torch.exp(log_qzx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7881)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence = log_qzx - log_pz\n",
    "kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob qzx:  tensor(-5.8899) prob: tensor(0.0028)\n"
     ]
    }
   ],
   "source": [
    "# now, if we manually move q closer to p, we see that this distrance has shrunk.\n",
    "q_new = torch.distributions.Normal(0.1, 1.1)\n",
    "\n",
    "log_qzx_new = q_new.log_prob(z)\n",
    "print('log prob qzx: ', log_qzx_new, 'prob:', torch.exp(log_qzx_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2771)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kl_divergence_new = log_qzx_new - log_pz\n",
    "new_kl_divergence_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<font color=black>\n",
    "\n",
    "- Now, this z has a single dimension. But in the real world, we care about n-dimensional zs. To handle this in the implementation, we simply sum over the last dimension. \n",
    "- The **trick here** is that when sampling from a univariate distribution (in this case Normal), if you sum across many of these distributions, itâ€™s equivalent to using an n-dimensional distribution (n-dimensional Normal in this case). \n",
    "- This generic form of the KL is called the **monte-carlo approximation**. This means we sample z many times and estimate the KL divergence. (in practice, these estimates are really good and with a batch size of 128 or more, the estimate is very accurate)\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, enc_out_dim=512, latent_dim=256, input_height=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # encoder, decoder\n",
    "        self.encoder = resnet18_encoder(False, False)\n",
    "        self.decoder = resnet18_decoder(\n",
    "            latent_dim=latent_dim,\n",
    "            input_height=input_height,\n",
    "            first_conv=False,\n",
    "            maxpool1=False\n",
    "        )\n",
    "\n",
    "        # distribution parameters\n",
    "        self.fc_mu = nn.Linear(enc_out_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(enc_out_dim, latent_dim)\n",
    "\n",
    "        # for the gaussian likelihood\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def gaussian_likelihood(self, mean, logscale, sample):\n",
    "        scale = torch.exp(logscale)\n",
    "        dist = torch.distributions.Normal(mean, scale)\n",
    "        log_pxz = dist.log_prob(sample)\n",
    "        return log_pxz.sum(dim=(1, 2, 3))\n",
    "\n",
    "    def kl_divergence(self, z, mu, std):\n",
    "        # --------------------------\n",
    "        # Monte carlo KL divergence\n",
    "        # --------------------------\n",
    "        # 1. define the first two probabilities (in this case Normal for both)\n",
    "        p = torch.distributions.Normal(\n",
    "            torch.zeros_like(mu), torch.ones_like(std))\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        # 2. get the probabilities from the equation\n",
    "        log_qzx = q.log_prob(z)\n",
    "        log_pz = p.log_prob(z)\n",
    "\n",
    "        # kl\n",
    "        kl = (log_qzx - log_pz)\n",
    "        kl = kl.sum(-1)\n",
    "        return kl\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # encode x to get the mu and variance parameters\n",
    "        x_encoded = self.encoder(x)\n",
    "        mu, log_var = self.fc_mu(x_encoded), self.fc_var(x_encoded)\n",
    "\n",
    "        # sample z from q\n",
    "        std = torch.exp(log_var / 2)\n",
    "        q = torch.distributions.Normal(mu, std)\n",
    "        z = q.rsample()\n",
    "\n",
    "        # decoded\n",
    "        x_hat = vae.decoder(z)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = self.gaussian_likelihood(x_hat, self.log_scale, x)\n",
    "\n",
    "        # kl\n",
    "        kl = self.kl_divergence(z, mu, std)\n",
    "\n",
    "        # elbo\n",
    "        elbo = (kl - recon_loss)\n",
    "        elbo = elbo.mean()\n",
    "\n",
    "        self.log_dict({\n",
    "            'elbo': elbo,\n",
    "            'kl': kl.mean(),\n",
    "            'recon_loss': recon_loss.mean(),\n",
    "            'reconstruction': recon_loss.mean(),\n",
    "            'kl': kl.mean(),\n",
    "        })\n",
    "\n",
    "        return elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(1234)\n",
    "\n",
    "vae = VAE()\n",
    "trainer = pl.Trainer(gpus=0, max_epochs=30, progress_bar_refresh_rate=10)\n",
    "trainer.fit(vae, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(8, 3), dpi=300)\n",
    "\n",
    "# Z COMES FROM NORMAL(0, 1)\n",
    "num_preds = 16\n",
    "p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "z = p.rsample((num_preds,))\n",
    "\n",
    "# SAMPLE IMAGES\n",
    "with torch.no_grad():\n",
    "    pred = vae.decoder(z.to(vae.device)).cpu()\n",
    "\n",
    "# UNDO DATA NORMALIZATION\n",
    "normalize = cifar10_normalization()\n",
    "mean, std = np.array(normalize.mean), np.array(normalize.std)\n",
    "img = make_grid(pred).permute(1, 2, 0).numpy() * std + mean\n",
    "\n",
    "# PLOT IMAGES\n",
    "imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up the repository\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.rm(\"cifar-10-python.tar.gz\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<font color=black>\n",
    "\n",
    "- Some things may not be obvious still from this explanation. First, each image will end up with its own q. The KL term will push all the qs towards the same p (called the prior). But if all the qs, collapse to p, then the network can cheat by just mapping everything to zero and thus the VAE will collapse.\n",
    "\n",
    "- The reconstruction ter, forces each `q` to be uniqye and spread iut so taht the image can be reconstructed correctly. This keeps all the qs from collapsing onto each other.\n",
    "\n",
    "- This is done in a way to seek a balance. This is also why you may experience **instability** in training VAEs! GANs?\n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<font color=black>\n",
    "\n",
    "- https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
    "- https://github.com/williamFalcon/pytorch-lightning-vae  \n",
    "- https://colab.research.google.com/drive/1_yGmk8ahWhDs23U4mpplBFa-39fsEJoT?usp=sharing#scrollTo=EYDKIsTtk3hJ\n",
    "- https://github.com/ethen8181/machine-learning/blob/master/model_selection/kl_divergence.ipynb \n",
    "\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
